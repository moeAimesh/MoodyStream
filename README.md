ğŸ§  Moody â€“ Emotion- & Gesten-Erkennung mit Soundreaktion
ğŸ“– Projektbeschreibung

Moody ist ein KI-gestÃ¼tztes System, das mithilfe einer Webcam das Gesicht und Gesten einer Person analysiert.
Das Ziel: Das Programm erkennt Emotionen oder Bewegungen (z. B. Daumen hoch) und reagiert darauf mit individuell zugeordneten Sounds.

Das System besteht aus drei Hauptphasen:

Setup-Phase: Nutzer kalibriert seine Gesichtsemotionen und wÃ¤hlt passende Sounds aus.

Live-Erkennungsphase: Kamera erkennt Emotionen & Gesten in Echtzeit.

Reaktionsphase: Das System spielt die passenden Sounds ab.

ğŸ“ Projektstruktur

emotion_sound_ai/
â”‚
â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ setup/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ setup_wizard.py
â”‚   â”œâ”€â”€ face_setup.py
â”‚   â”œâ”€â”€ sound_setup.py
â”‚   â”œâ”€â”€ setup_config.json
â”‚   â””â”€â”€ profiles/
â”‚       â””â”€â”€ default_face_baseline.json
â”‚
â”œâ”€â”€ detection/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ camera_stream.py
â”‚   â”œâ”€â”€ gesture_recognition.py
â”‚   â”œâ”€â”€ face_analyzer.py
â”‚   â”œâ”€â”€ emotion_recognition.py
â”‚   â””â”€â”€ detectors/
â”‚       â””â”€â”€ thumbs_up.py
â”‚
â”œâ”€â”€ sounds/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ play_sound.py
â”‚   â”œâ”€â”€ sound_cache/
â”‚   â””â”€â”€ sound_map.json
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_manager.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ settings.py
â”‚
â”œâ”€â”€ ai/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_loader.py
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ icons/
â”‚   â”œâ”€â”€ sounds/
â”‚   â””â”€â”€ themes/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


âš™ï¸ ErklÃ¤rung der Module
ğŸ§­ main.py

Einstiegspunkt des Projekts.

FÃ¼hrt den gesamten Ablauf aus:

Startet den Setup-Wizard (Gesicht + Sounds).

Startet danach die Live-Erkennung mit Kamera.

Erkennt, ob ein Profil bereits existiert, um Setup ggf. zu Ã¼berspringen.

ğŸ§© Setup-Phase (setup/-Ordner)
ğŸ”¹ setup/setup_wizard.py

Steuert den gesamten Einrichtungsablauf.

Ruft nacheinander face_setup.py und sound_setup.py auf.

Gibt am Ende True zurÃ¼ck, wenn alles erfolgreich abgeschlossen wurde.


ğŸ”¹ setup/face_setup.py
FÃ¼hrt eine Gesichtskalibrierung mit DeepFace durch.

LÃ¤dt das Emotion-Modell einmalig und nutzt es fÃ¼r mehrere Aufnahmen.

Der Nutzer sieht ein Live-Kamerabild und drÃ¼ckt ENTER, um emotionale Snapshots zu speichern.

Der Durchschnitt dieser Emotionen wird als Baseline (Ruhegesicht) gespeichert:

setup/profiles/default_face_baseline.json

und in setup/setup_config.json unter dem Key "faces"


ğŸ”¹ setup/sound_setup.py

Ã–ffnet eine integrierte Browser-Ansicht mit myinstants.com.

Nutzer klickt dort manuell auf Sounds (mp3-Dateien).

Das System fÃ¤ngt den Download-Link ab, lÃ¤dt die Datei in den Cache und fragt:

â€Welchem Verhalten soll dieser Sound zugeordnet werden?â€œ
(z. B. ok, laugh, angry, thumbsup)

Speichert Zuordnung in:

sounds/sound_cache/ (Dateien)

sounds/sound_map.json

setup/setup_config.json

Zentrale Konfigurationsdatei, die Setup-Ergebnisse speichert.

EnthÃ¤lt z. B.:

{
  "faces": { "happy": 0.52, "neutral": 0.33, "sad": 0.15 },
  "sounds": { "ok": "sounds/sound_cache/ok.mp3" }
}


ğŸ”¹ setup/profiles/

Speichert die individuellen Face-Baseline-Dateien je Nutzer.

Format: <username>_face_baseline.json




ğŸ¥ Detection-Phase (detection/-Ordner)
ğŸ”¹ camera_stream.py

Ã–ffnet die Webcam und lÃ¤uft in einer Endlosschleife.

Ruft pro Frame:

gesture_recognition.py â†’ erkennt Gesten (z. B. Daumen hoch)
face_analyzer.py â†’ analysiert Emotionen

Wenn eine bekannte Emotion oder Geste erkannt wird:
â†’ spielt Ã¼ber sounds/play_sound.py den passenden Sound ab.


ğŸ”¹ gesture_recognition.py

Nutzt MediaPipe (mp.solutions.hands), um Handpositionen zu tracken.

Erkennt definierte Gesten Ã¼ber detectors/thumbs_up.py.

Gibt z. B. "thumbsup" oder None zurÃ¼ck.
bei weiteren Gesten Weitere dateien hinzufÃ¼gen (z. B. Peace.py, Wave.py, Fist.py) die Ã¤hnluch funkitonieren sollen wie thumbsup.py

ğŸ”¹ detectors/thumbs_up.py

EnthÃ¤lt reine Logik zur Erkennung eines â€Daumen hochâ€œ.

Wird von gesture_recognition.py genutzt.

RÃ¼ckgabe: True oder False.



ğŸ”¹ face_analyzer.py

Nutzt DeepFace, um Emotionen im Livebild zu erkennen.
Vergleicht aktuelle Werte mit der gespeicherten Face-Baseline.
Liefert erkannte Emotion zurÃ¼ck (z. B. "happy").

ğŸ”¹ emotion_recognition.py

Kombiniert Gesichtsergebnisse und Gestenergebnisse.

WÃ¤hlt anhand der PrioritÃ¤t(muss definiert werden) welcher Sound gespielt wird.




ğŸ”Š Sound-System (sounds/-Ordner)
ğŸ”¹ play_sound.py

Nutzt pygame.mixer zum Abspielen von Sounds.

Bietet Funktionen wie:

play(path, volume=1.0)
stop_all()

Spielt Sounds aus sounds/sound_map.json ab, basierend auf dem erkannten Verhalten.

ğŸ”¹ sound_cache/

Lokaler Speicher fÃ¼r heruntergeladene mp3-Dateien.

ğŸ”¹ sound_map.json

EnthÃ¤lt die Zuordnung Emotion/Geste â†’ Soundpfad.
Beispiel:

{
  "thumbsup": "sounds/sound_cache/ok.mp3",
  "happy": "sounds/sound_cache/laugh.mp3"
}


ğŸ§° Hilfsmodule (utils/-Ordner)
ğŸ”¹ json_manager.py

Einfaches Laden und Speichern von JSON-Dateien.

Verhindert Fehler, wenn Datei leer oder defekt ist.

ğŸ”¹ settings.py

Zentrale Pfaddefinitionen und globale Konstanten (z. B. BASE_DIR, CACHE_PATH).

ğŸ§  Moody â€“ Emotion- & Gesten-Erkennung mit Soundreaktion

ğŸ“– Projektbeschreibung

Moody ist ein KI-gestÃ¼tztes System, das mithilfe einer Webcam das Gesicht und Gesten einer Person analysiert.
Das Ziel: Das Programm erkennt Emotionen oder Bewegungen (z. B. Daumen hoch) und reagiert darauf mit individuell zugeordneten Sounds.

Das System besteht aus drei Hauptphasen:

Setup-Phase: Nutzer kalibriert seine Gesichtsemotionen und wÃ¤hlt passende Sounds aus.

Live-Erkennungsphase: Kamera erkennt Emotionen & Gesten in Echtzeit.

Reaktionsphase: Das System spielt die passenden Sounds ab.



ğŸ“ Projektstruktur (wird sich safe bisschen Ã¤ndern)


```mermaid
graph TD

A[main.py] --> B[setup/]
A --> C[detection/]
A --> D[sounds/]
A --> E[utils/]
A --> F[ai/]
A --> G[assets/]

B --> B1[setup_wizard.py]
B --> B2[face_setup.py]
B --> B3[sound_setup.py]
B --> B4[setup_config.json]
B --> B5[profiles/]
B5 --> B6[default_face_baseline.json]

C --> C1[camera_stream.py]
C --> C2[gesture_recognition.py]
C --> C3[face_analyzer.py]
C --> C4[emotion_recognition.py]
C --> C5[detectors/]
C5 --> C6[thumbs_up.py]

D --> D1[play_sound.py]
D --> D2[sound_cache/]
D --> D3[sound_map.json]

E --> E1[json_manager.py]
E --> E2[logger.py]
E --> E3[settings.py]

F --> F1[model_loader.py]

G --> G1[icons/]
G --> G2[sounds/]
G --> G3[themes/]
```


âš™ï¸ ErklÃ¤rung der Module


ğŸ§­ main.py


Einstiegspunkt des Projekts.

FÃ¼hrt den gesamten Ablauf aus:

Startet den Setup-Wizard (Gesicht + Sounds).

Startet danach die Live-Erkennung mit Kamera.

Erkennt, ob ein Profil bereits existiert, um Setup ggf. zu Ã¼berspringen.

ğŸ§© Setup-Phase (setup/-Ordner)


ğŸ”¹ setup/setup_wizard.py

Steuert den gesamten Einrichtungsablauf.

Ruft nacheinander face_setup.py und sound_setup.py auf.

Gibt am Ende True zurÃ¼ck, wenn alles erfolgreich abgeschlossen wurde.


ğŸ”¹ setup/face_setup.py
FÃ¼hrt eine Gesichtskalibrierung mit DeepFace durch.

LÃ¤dt das Emotion-Modell einmalig und nutzt es fÃ¼r mehrere Aufnahmen.

Der Nutzer sieht ein Live-Kamerabild und drÃ¼ckt ENTER, um emotionale Snapshots zu speichern.

Der Durchschnitt dieser Emotionen wird als Baseline (Ruhegesicht) gespeichert:

setup/profiles/default_face_baseline.json

und in setup/setup_config.json unter dem Key "faces"


ğŸ”¹ setup/sound_setup.py

Ã–ffnet eine integrierte Browser-Ansicht mit myinstants.com.

Nutzer klickt dort manuell auf Sounds (mp3-Dateien).

Das System fÃ¤ngt den Download-Link ab, lÃ¤dt die Datei in den Cache und fragt:

â€Welchem Verhalten soll dieser Sound zugeordnet werden?â€œ
(z. B. ok, laugh, angry, thumbsup)

Speichert Zuordnung in:

sounds/sound_cache/ (Dateien)

sounds/sound_map.json

setup/setup_config.json

Zentrale Konfigurationsdatei, die Setup-Ergebnisse speichert.

EnthÃ¤lt z. B.:

{
  "faces": { "happy": 0.52, "neutral": 0.33, "sad": 0.15 },
  "sounds": { "ok": "sounds/sound_cache/ok.mp3" }
}


ğŸ”¹ setup/profiles/

Speichert die individuellen Face-Baseline-Dateien je Nutzer.

Format: <username>_face_baseline.json




ğŸ¥ Detection-Phase (detection/-Ordner)


ğŸ”¹ camera_stream.py

Ã–ffnet die Webcam und lÃ¤uft in einer Endlosschleife.

Ruft pro Frame:

gesture_recognition.py â†’ erkennt Gesten (z. B. Daumen hoch)
face_analyzer.py â†’ analysiert Emotionen

Wenn eine bekannte Emotion oder Geste erkannt wird:
â†’ spielt Ã¼ber sounds/play_sound.py den passenden Sound ab.


ğŸ”¹ gesture_recognition.py

Nutzt MediaPipe (mp.solutions.hands), um Handpositionen zu tracken.

Erkennt definierte Gesten Ã¼ber detectors/thumbs_up.py.

Gibt z. B. "thumbsup" oder None zurÃ¼ck.
bei weiteren Gesten Weitere dateien hinzufÃ¼gen (z. B. Peace.py, Wave.py, Fist.py) die Ã¤hnluch funkitonieren sollen wie thumbsup.py


ğŸ”¹ detectors/thumbs_up.py

EnthÃ¤lt reine Logik zur Erkennung eines â€Daumen hochâ€œ.

Wird von gesture_recognition.py genutzt.

RÃ¼ckgabe: True oder False.



ğŸ”¹ face_analyzer.py

Nutzt DeepFace, um Emotionen im Livebild zu erkennen.
Vergleicht aktuelle Werte mit der gespeicherten Face-Baseline.
Liefert erkannte Emotion zurÃ¼ck (z. B. "happy").

ğŸ”¹ emotion_recognition.py

Kombiniert Gesichtsergebnisse und Gestenergebnisse.

WÃ¤hlt anhand der PrioritÃ¤t(muss definiert werden) welcher Sound gespielt wird.




ğŸ”Š Sound-System (sounds/-Ordner)


ğŸ”¹ play_sound.py

Nutzt pygame.mixer zum Abspielen von Sounds.

Bietet Funktionen wie:

play(path, volume=1.0)
stop_all()

Spielt Sounds aus sounds/sound_map.json ab, basierend auf dem erkannten Verhalten.

ğŸ”¹ sound_cache/

Lokaler Speicher fÃ¼r heruntergeladene mp3-Dateien.

ğŸ”¹ sound_map.json

EnthÃ¤lt die Zuordnung Emotion/Geste â†’ Soundpfad.
Beispiel:

{
  "thumbsup": "sounds/sound_cache/ok.mp3",
  "happy": "sounds/sound_cache/laugh.mp3"
}


ğŸ§° Hilfsmodule (utils/-Ordner)


ğŸ”¹ json_manager.py

Einfaches Laden und Speichern von JSON-Dateien.

Verhindert Fehler, wenn Datei leer oder defekt ist.

ğŸ”¹ settings.py

Zentrale Pfaddefinitionen und globale Konstanten (z. B. BASE_DIR, CACHE_PATH).
